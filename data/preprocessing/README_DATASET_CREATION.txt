README

Generating the PTB datasets (without Gigaword) to replicate our paper is a 4-step process:

1. Download Treebank-3 (LDC99T42) from https://catalog.ldc.upenn.edu/LDC99T42. Unarchive LDC99T42.tgz and place the treebank_3 directory in the datasets/ directory in this repo.

2. From the bash command line, change the working directory to data/preprocessing (for example `cd data/preprocessing` from the repo root).

3. Run `bash create_dependency_files.sh`. This will make a copy of the WSJ constituency trees, patch the WSJ part of the Penn Treebank using the NP bracketing script by David Vadas, and convert the NP-bracketed constituency trees to dependency trees using the LTH Constituent-to-Dependency Conversion Tool for Penn-style Treebanks. This may take around 10 minutes on modern hardware.

4. Run `bash create_dataset.sh`. This will create the core dataset files. This includes ordered PTB files with and without base NP annotations. This will also generate the exact shuffling of the word multisets we used to generate the point estimates in our paper. This script also generates versions formatted for use with ZGen and the Yara parer. (The latter is used for post-hoc analysis in our paper.) This may take around 5 minutes on modern hardware.

Overview of the resulting preprocessing:

After following these steps, the folder `zgen_data_gold` will contain the gold, ordered sentences. The files ending in '_ref_npsyms.txt' include the base noun phrases (BNPs) we used (the inclusion of which was a holdover from the setup of previous papers in this line of research).

BNPs are marked with:
    a start symbol: <sonp>
    an end symbol:  <eonp>

Those same files with the BNPs removed, which were used for our WORDS models, are also included in the `zgen_data_gold` folder. In calculating BLEU, use the reference files that do not have BNPs.

The folder `zgen_data_npsyms_freq3_unkUNK` contains the files with low frequency tokens replaced with special symbols that were used for the
results in our paper. Shuffled versions are also included. (Note that as with past work, in the case of BNPs, when shuffling the bag of words
for input, the BNPs are retained as atomic units. In other words, the tokens within the BNP symbols are not shuffled. For the non-BNP files,
the tokens are fully shuffled.)

Additional Notes:

The above has been most recently tested on OSX 10.11.5. In principle, it should also run on Linux variants. 

Dependencies:

Python: Most recently tested with 2.7.9 
NLTK (Python package): Most recently tested with 3.0.4
Java: Most recently tested with 1.8.0_31

Gigaword:

We used version 1.2 of the Annotated Gigaword API and Command Line Tools available at https://github.com/mgormley/agiga.

The exact lines appearing in our 900k sample are provided in the key available at data/preprocessing/gigaword/gigaword_key/afp_900k_key.txt.

At a high-level, the scripts in the gigaword directory aim to:
-Convert the Annotated Gigaword xml files to a bracketed tree format
-Convert the bracketed tree format to tokenized sentences with and without base NPs
-Allow the total vocabulary size of the Gigaword and PTB concatenation to rise to around 25k.

More specifically, to replicate the data we used to generate our results using Gigaword, perform the following steps:

1. Download and unarchive Annotated English Gigaword (https://catalog.ldc.upenn.edu/LDC2012T21). Download version 1.2 of the Annotated Gigaword API and Command Line Tools (agiga) available at https://github.com/mgormley/agiga.

2. Use the script gigaword/gigaword_conversion_stanford_phrase_structure_afp.sh to construct the constituency trees from the XML. You will need to change the variable DATA_DIR to point to the xml directory of Gigaword. OUTPUT_DIR should point to a directory for saving the trees and LOGS_DIR saves logging information. The script should be run from the agiga repo (or change the path to the Java files accordingly). 

3. Next, use the script gigaword/gigaword_conversion_phrase_structure_npsyms_afp.sh to create files with base np annotations from the trees generated by the previous step. Change the variables at the top of the file based on the locations chosen for the previous step. In particular, DATA_DIR should be the OUTPUT_DIR from Step 2 above. The new OUTPUT_DIR (and associated logs at LOGS_DIR) will contain all files in the AFP section converted to sentences with base NP annotations (files ending in _processed.txt). (The aforementioned gigaword_key/afp_900k_key.txt contains samples of the _processed.txt files.)

4. The script gigaword/gigaword_create_splits.py will create the 900k sample. --input_dir should be the OUTPUT_DIR from step 3 above.

5. Next, the script gigaword/gigaword_create_splits_tokenize.py merges the PTB training with the Gigaword sample. The vocab is expanded beyond that of datasets/zgen_data_npsyms_freq3_unkUNK/npsyms/train_words_with_np_symbols_no_eos.txt with additional types from Gigaword (up to around 25k total). Base NPs can be removed from the resulting output files with remove_base_npsyms.py in the preprocessing directory.

6. Finally, the script gigaword/gigaword_shuffle.py can be used to shuffle the Gigaword validation and test datasets. The resulting files are shuffled to match the shuffling of the 
datasets/zgen_data_npsyms_freq3_unkUNK datasets, but removes unk/UNK symbols within the 25K Gigaword vocabulary. See --help for input file naming expectations. The preprocessing/remove_eos.py script can subsequently be used to remove eos symbols (since the current versions of the decoders expect shuffled input without explicit eos symbols).